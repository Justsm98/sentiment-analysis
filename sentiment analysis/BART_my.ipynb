{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b4b15b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe BART model is designed to address some of the limitations of previous transformer-based models, \\nsuch as the inability to handle bidirectional input and the lack of a pre-training method for \\nsequence-to-sequence models. BART uses a combination of bidirectional and autoregressive training to \\nachieve better performance on a range of NLP tasks.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The BART model is designed to address some of the limitations of previous transformer-based models, \n",
    "such as the inability to handle bidirectional input and the lack of a pre-training method for \n",
    "sequence-to-sequence models. BART uses a combination of bidirectional and autoregressive training to \n",
    "achieve better performance on a range of NLP tasks.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a747523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BartTokenizer, TFBartForSequenceClassification\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c47beaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBartForSequenceClassification: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight']\n",
      "- This IS expected if you are initializing TFBartForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBartForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBartForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classification_head.dense.weight', 'classification_head.dense.bias', 'classification_head.out_proj.weight', 'classification_head.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer and the pre-trained BART model\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base',do_lower_case=True)      #####May have loading problem here\n",
    "model = TFBartForSequenceClassification.from_pretrained('facebook/bart-base', num_labels = 5,from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess the data\n",
    "import pandas as pd\n",
    "\n",
    "def load_data(split_name='train', columns=['text', 'stars'], folder='data'):\n",
    "    '''\n",
    "        \"split_name\" may be set as 'train', 'valid' or 'test' to load the corresponding dataset.\n",
    "        \n",
    "        You may also specify the column names to load any columns in the .csv data file.\n",
    "        Among many, \"text\" can be used as model input, and \"stars\" column is the labels (sentiment). \n",
    "        If you like, you are free to use columns other than \"text\" for prediction.\n",
    "    '''\n",
    "    try:\n",
    "        print(f\"select [{', '.join(columns)}] columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        df = df.loc[:,columns]\n",
    "        print(\"Success\")\n",
    "        return df\n",
    "    except:\n",
    "        print(f\"Failed loading specified columns... Returning all columns from the {split_name} split\")\n",
    "        df = pd.read_csv(f'{folder}/{split_name}.csv')\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48549df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select [text, stars] columns from the train split\n",
      "Success\n",
      "select [text, stars] columns from the valid split\n",
      "Success\n",
      "select [text, stars] columns from the test split\n",
      "Failed loading specified columns... Returning all columns from the test split\n"
     ]
    }
   ],
   "source": [
    "train_df = load_data('train', columns=['text', 'stars'], folder='data')\n",
    "valid_df = load_data('valid', columns=['text', 'stars'], folder='data')\n",
    "# the test set labels (the 'stars' column) are not available! So the following code will instead return all columns\n",
    "test_df = load_data('test', columns=['text', 'stars'], folder='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "562b4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data.\n",
    "# As an example, we only use the text data. \n",
    "train_df = train_df.sample(frac=1).reset_index(drop=True)\n",
    "valid_df = valid_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "x_train = train_df['text']\n",
    "y_train = train_df['stars']\n",
    "  \n",
    "x_valid = valid_df['text']\n",
    "val_labels = valid_df['stars']\n",
    "\n",
    "x_test = test_df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca14741",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text = train_df.text.values\n",
    "train_text = train_text[:10000]\n",
    "train_text_cut = []\n",
    "for sentence in train_text:\n",
    "    if len(sentence) > 126:\n",
    "        first_part = sentence[:64]\n",
    "        second_part = sentence[-62:]\n",
    "        train_text_cut.append(first_part + second_part)\n",
    "    else:\n",
    "        train_text_cut.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8685f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_text = valid_df.text.values\n",
    "# val_text = val_text[:800]\n",
    "val_text_cut = []\n",
    "for sentence in val_text:\n",
    "    if len(sentence) > 126:\n",
    "        first_part = sentence[:64]\n",
    "        second_part = sentence[-62:]\n",
    "        val_text_cut.append(first_part + second_part)\n",
    "    else:\n",
    "        val_text_cut.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22fb25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sentences(sentences):\n",
    "    # Tokenize the sentences\n",
    "    input_ids = [tokenizer.encode(sent, add_special_tokens=True) for sent in sentences]\n",
    "    # Pad the tokenized sentences\n",
    "    input_ids = tf.keras.preprocessing.sequence.pad_sequences(input_ids, \n",
    "                                                              value=0, \n",
    "                                                              padding='post', \n",
    "                                                              maxlen=128)  \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bac4c250",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_ids = encode_sentences(train_text_cut)\n",
    "val_input_ids = encode_sentences(val_text_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fc66744",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = y_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e55b2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  98/1500 [>.............................] - ETA: 7:18:14 - loss: 1.5365 - accuracy: 0.3865"
     ]
    }
   ],
   "source": [
    "# Fine-tune the BART model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=loss, \n",
    "              metrics=[metric])\n",
    "\n",
    "model.fit(train_input_ids, train_labels-1, \n",
    "          epochs=1, batch_size=8)# epoch2 haopiaohua, overfitting?danhaoxianghaizaishangsheng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f971860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = model.predict(val_input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31953bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre = tf.argmax(y_pre.logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb95b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "print(classification_report(val_labels-1, y_pre))\n",
    "print(\"\\n\\n\")\n",
    "print(confusion_matrix(val_labels-1, y_pre))\n",
    "#print('accuracy', np.mean(val_labels-1 == y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2304393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "#test_predictions = model.predict(test_input_ids)\n",
    "#test_ratings = tf.argmax(test_predictions, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
